---
layout: post
title: Seastar Vertex-Centric Programming for Graph Neural Networks
---

{{ page.title }}
================

<p class="meta">18 May 2022 - Wuhan</p>
# 摘要

图神经网络在节点分类、链接预测、图聚类等图分析领域取得了突破性的进展。已经开发了许多GNN培训框架，但它们通常被设计为一组手工编写的、特定于GNN的操作符，插入到现有的深度学习系统中，这导致内存消耗高、数据局域性差、算法设计和实现之间的语义差距大。本文提出了**Seastar系统，该系统提出了一种在GPU上进行GNN训练的以顶点为中心的编程模型，并提供了惯用的python构造，使开发新的同构和异构GNN模型变得容易**。我们还提出了新的优化方法，**以产生高效的融合GPU核**，**用于GNN训练中的前向和后向传递**。与目前最先进的GNN系统DGL和PyG相比，海星的可用性更好，内存消耗分别减少了2倍和8倍，执行速度分别提高了14倍和3倍



# 1 引入

以最先进的GNN框架DGL为例。DGL提供了一个图抽象，使用消息传递原语对图卷积步骤建模，并将原语实现为插入到现有DL系统中的操作符(例如，TensorFlow [2]， PyTorch [50]， MxNet[16])。在运行时，GNN运算符可以被DL系统视为普通的DL运算符，DL系统的自动微分模块处理反向传播。GNN框架可以在不修改底层DL系统的情况下设计有效的消息传递原语。

## 问题

首先，现有的GNN框架使用了以**全图张量为中心的编程模型**。该编程模型旨在将GNN算子实现为DL系统的插件，但这使得**GNN模型的实际实现难以遵循模型的概念设计**，特别是涉及到**图操作**时。

其次，现有的设计暴露了运营商的通用性和性能优化之间的权衡。有些**GNN框架倾向于通用性**，例如[3,22,44,68]将GNN计算分解为**散/聚等图传播原语**，并通过中间张量物化与其他DL运算符连接，导致内存消耗过大，数据移动量大。相比之下，DGL[59]为一些**常用的算子组合提供了融合核，但融合核的通用性有限**，DGL的性能仍然不是最优的

seastar框架，用户只需要用熟悉的Python语法编写一个顶点的逻辑。为了支持各种算子组合的高效执行，我们在**训练同质和异构GNN模型时确定了一个类似的海星执行模式**

基于海星模式，我们设计了一个通用的核发生器(§5)，它为**前进和后退通道产生高效的融合核**。

我们提出了**海星算子融合和内核级优化**(§6.3)，如特性自适应线程映射、以位置为中心的执行和动态负载平衡

# 2 background and motivation

## 2.1 GNN

graph g，边u->v, uv。gnn每一层，可以表示成

![image-20220525174900267](https://alittleasewolf.github.io/imgs/image-20220525174900267.png)

$h_v^{l}$ 是 第$l$层顶点$v$的特征，$f,g$都是神经网络模型和函数，$ \oplus $代表聚合顶点$v$的邻居和边的特征

## 2.2 GNN编程模型的限制

虽然现有的GNN培训框架提供了各种类型的编程模型，如**消息传递和本地数据流编程**，但它们共享一个以**全图张量为中心**的范式，因为这些框架中的操作符的粒度是高维张量。

为了访问**邻居的特征**，用户使用**消息传递/分散操作显式或隐式地创建边缘张量**，并且必须仔细跟踪维数，以便在正确的维数上进行约简。数据流编程模型有更底层的接口，用户需要关系底层细节，例如 一条边的uv的ID，以及使用操作符例如scatter和segment_sum来实现消息和reduce函数。

以**全图张量为中心的设计**简化了与现有的DL系统的集成，因为数据和操作的粒度与DL系统一致，但给用户带来了繁重的任务，即将方程1中的局部计算转换为全局张量操作。这应该由GNN培训框架来处理，以获得更好的可用性和更高的用户生产力

## 2.3 现有优化的局限

大多数现有的消息传递系统[3,22,44,68]将创建消息时产生的**消息张量和中间结果物化**，这导致了高内存消耗和数据在流多处理器(SMs)和设备内存之间移动。在这些系统中，内存消耗与特征大小以及顶点和边的数量成正比。这极大地限制了它们的可伸缩性

**算子融合将连续的算子组合成一个核**。在融合内核中，操作符直接使用寄存器将结果传递给下游操作符，而不将结果转储到全局内存中，从而节省了全局内存消耗和执行时间。然而，将算子融合应用到GNN运算中并非易事。现有的**DL系统只能生成简单的融合核**，如融合多个单元操作，通常需要人工开发高性能核如fusion conv2d-element-wise[2,50]。TVM[17]等DL编译器将运算符分为四类:单射、约简、复出融合和不透明(不融合)，并设计了它们的融合规则。基于多面体模型的DL编译器[5,57]可以实现算子之间的融合，实现自动多面体变换。然而，由于图结构数据的不规则性，**GNN由不规则计算组成，需要依赖于数据的数据访问和聚合**，这超出了现有的DL编译器的能力。事实上，识别和融合复杂模式对于DL编译器来说仍然是一个普遍的难题。

DGL[59]为常见模式提供定制优化。DGL将一个**沿边的操作(例如，添加一条边的两端顶点的特性)与一个边的聚合**(例如，sum)合并到一个融合的**BinaryReduce内核**中。通过避免将二进制运算的结果物化(通常是一个大小与边数成比例的张量)，它实现了显著的内存节省。然而，BinaryReduce在整个GNN设计空间中只代表了一个很小的算子组合子集，并且由于它的内核设计，性能很差，我们将在§7中展示。最近的一项工作[31]提出了广义**SpMM(备用密集矩阵乘法)**和**SDDMM(样本密集矩阵乘法**)，并使用**TVM[17]生成核**。然而，用户需要使用**TVM的计算和调度原语编程来开发新的GNN层**，这需要很好的理解GPU的架构和底层图形数据表示细节。

# 3 概述

Seastar与现有GNN培训框架的区别主要在于它的以**顶点为中心的编程模型(为了更好的可用性)和执行计划生成(为了更高的性能)**

Seastar的编程模型允许用户使用**以顶点为中心的用户定义函数(udf)轻松地编程GNN模型**。

在§4中，我们介绍了这种以顶点为中心的编程模型，并以**GCN和GAT为例将其与DGL的张量为中心的编程模型进行了比较**，以突出其优点。

给定一个以顶点为中心的UDF, Seastar生成一个高效的执行计划(§5)，**并应用算子融合和内核级优化**(§6)。Seastar首先通过一个**示踪器将以顶点为中心的逻辑转换为张量操作**(§5)，然后通过**Seastar计算模式的概念识别算子融合**机会。海星通过**特性可适应的线程分组**(§6.3.1)和**以位置为中心的执行**(§6.3.2)，有效地利用了**GPU的大量并行性**，这界定了海星与其他解决方案(如DGL)在SIMT并行化策略方面的关键差异。海星还支持**动态负载平衡来处理倾斜负载**(§6.3.3)。

# 4 顶点为中心的GNN编程

以顶点为中心的GNN编程模型，源自pregel的以顶点为中心的分布式图计算模型。我们的目标是更自然的GNN编程，以便用户的学习曲线是平坦的。

可以观察到方程1中采用了**以顶点为中心的计算形式**，通过聚合邻居的特征来计算中心顶点的特征。

为了支持以顶点为中心的编程，我们引入了一个函数装饰器compile。——只有一个参数v，用户只需要对v进行编程操作即可。用户只需要在函数`vc_compute(v)`之前包含`@Seastar.compile()`，然后seastar就会编译vc_compute(v)然后对输入图的所有顶点进行此操作。可以使用python语法，比如dot来访问邻居特征，列表推导和约简对邻居特征进行计算和聚合。

![image-20220525204102259](https://alittleasewolf.github.io/imgs/image-20220525204102259.png)

图3展示了GCN和GAT如何在seastar里面实现的，GCN的核心步骤可以简单的被一步表示`sum([torch.mm(u.h,self.w)*u.norm for u in v.innbs])`.更重要的是，我们可以看到图1和图2中的GNN公式与图3中以顶点为中心的实现(即vc_compute(v))之间的明确对应关系。好处是双向的:用户可以很容易地实现GNN模型，用户可以通过直接查看GNN模型的实现来学习GNN模型。

![image-20220525204411504](https://alittleasewolf.github.io/imgs/image-20220525204411504.png)

考虑DGL中的实现，line5存储h，line6执行整图的消息传递，line8检索聚合结果。与图3中sestar的GCN实现相比，上述GCN在DGL中的实现更为复杂，因为大量使用了专门的函数(fn.copy_src, graph.update_all, fn.edge_softmax)，另外，参照图1中的GCN公式，在图4中很难找到GCN公式与DGL的GCN代码的对应关系。另外，DGL对这些运算符逐个执行，并实现运算符之间的中间结果。相比之下，Seastar将运算符定义与运算符执行分开，这允许我们识别模式(从UDF、vc_compute(v))，用于显著提高训练吞吐量的内核级优化(§6)。我们在表1中总结了一些海星API。这个API只包含**编译修饰器以及顶点和边的属性**，因为用户可以在代码中简单地使用Python语法。有了这个简单的API，我们已经可以实现PYG和DGL支持的大多数同类和异类GNN模型。

![image-20220525204623424](https://alittleasewolf.github.io/imgs/image-20220525204623424.png)

![image-20220525205820847](https://alittleasewolf.github.io/imgs/image-20220525205820847.png)

# 5 Seastar 代码

本节描述了Seastar的代码生成过程，即Seastar如何将一个以顶点为中心的程序转换为一个由张量运算组成的计算图，以及海星如何集成了DL后台的运行时。

**seastar的主要DL后台是MindSpore**, MindSpore具有**自动并行训练功能**，可简化和加速分布式模型训练，并利用张量编译技术实现最佳性能，这反过来也提高了使用Seastar进行GNN训练的性能。然而，Seastar将其**代码生成和编译过程从后端DL系统中解耦**，它还可以与其他流行的DL后端集成。

这个设计是由以下原因驱动的。首先，我们发现与特定的DL后端(如MindSpore或PyTorch)合并将**需要在其中间表示和代码优化过程中进行许多更改**，这是一项艰巨的任务(对于合并和更新维护而言)，因为流行的DL系统正在积极升级.

在不同的DL后端中轻松地使用以顶点为中心的编程模型。例如，在我们的实验中，我们使用**PyTorch作为Seastar的DL后端，以便与DGL和PyG进行公平的比较**，因为它们都使用PyTorch作为DL后端。

![image-20220526094752623](https://alittleasewolf.github.io/imgs/image-20220526094752623.png)

图5显示了Seastar的主要组件和整个代码生成过程。在顶点中心函数中，使用§5.1中的**图形感知中间表示(GIR)对每个中心节点及其邻居进行跟踪和记录**。在§5.2中，我们为**GIR嵌入了一个自动分化引擎**。生成的代码将被编译并包装到一个通用的内核中，由DL后端执行，如§5.3所述。我们使用图3中的**GAT代码片段作为一个运行示例来说明每个组件**。

## 5.1 Tracer and GIR

### **操作符跟踪**

 与PyTorch的JIT跟踪器[50]类似，我们使用运算符重载来记录在以顶点为中心的函数中执行的操作。每个顶点和边缘的特征都可以通过使用“点”操作符使用vertex_feature和edge_feature字典中的键来访问。

每个特征向量都是一个符号张量，它继承了各种属性(例如，数据类型、形状、设备信息、是否需要梯度)从字典中给出的相应张量。对于形状属性，由于对应的张量沿第一个维分批处理顶点/边缘特征向量，我们剥离第一个维，并将其余维赋值为形状。**DL后端的所有操作符和张量的方法都将在decorator内部被补丁**。修补过的版本记录操作和它所操作的特性，并产生新的特性。修补后的版本还使用输入特性的属性创建张量，并使用原始版本执行操作符，以享受DL后端提供的类型推断。跟踪程序是一个DAG，因为跟踪本质上是展开循环和分支。然后在第一次运行后编译和缓存跟踪的dag，以便以后可以直接执行它。

GAT的顶点为中心的实现，用户提供了v_feature字典，**作为编译装饰器的参数。字典的值是tensors，每一行都被seastar解释为顶点的嵌入特征。**

当装饰函数(简称UDF)第一次被调用时，Seastar会为字典中每个key-value对中心顶点v以及他们的邻居u创建一个属性，属性名为key，属性的内容切分后的tensor(第一行tensor)，以反映属性属于单个顶点的事实。

注意，切片张量只在**示踪程序中使用**，**在第一次运行后将被丢弃**。然后，**将逐一调用UDF中的操作符**。除了对切片张量进行计算并作为普通运算符产生结果外，它们还利用自己的签名、输入和输出信息在全局DAG中创建一个节点。GAT的结果DAG在图6的正向GIR中显示。

![image-20220526104149352](https://alittleasewolf.github.io/imgs/image-20220526104149352.png)

### GIR

Tracer生成的DAG遵循DNN系统常用的计算图表示[2,16,50]。计算图中的节点表示操作，它以一个或多个张量作为输入，并产生新的张量作为输出。操作也可以具有诸如leakyRelu操作的斜率等属性。DAG指定了每个顶点的计算，这种方法可以对每个顶点单独执行DAG，但由于并行性有限，会严重损害GPU的性能。

Seastar将所有顶点的以顶点为中心的操作分组并一起执行。为此，Seastar用一个图类型注释了以顶点为中心的DAG中的张量， **$S(source) D(destination) E(edge) P(parameter)$ 意味着该张量对应于一行满张量，分别是源、目标和边的嵌入 ,张量是一个由所有顶点共享的参数(例如leakyRelu的斜率)**。张量的图类型可以由以顶点为中心的程序访问它的方式来确定

### Graph type inference

- 对于进行边向聚合的运算符(例如，GAT中的求和运算符)， input type S(D) 那么output type D(S). input 是E，那么返回类型由训练的方向决定，前向训练是D，反向是S。
- 对于接受单一图形类型输入的运算符，其输出张量与输入张量类型相同。
- 对于具有两种以上类型S，D，E的操作符，输出类型是E
- 当与其他图形类型一起用作输入时，类型P对输出的图形类型没有影响

### 运算符的图类型

我们根据运算符输出的图类型定义运算符的图类型，特别的是，operation定义为type S D E，如果他的输出为S D E。运算符的图类型总结了**执行该运算符的计算需要什么样的索引**。

- 对于S-type，顶点id被用作访问嵌入的索引。
- E-type，需要顶点id和/或边id来访问所有需要的嵌入，因为他的输入可能是SDE
- 此外，我们将聚合操作符的图类型(例如，GAT中的AggSum操作符)定义为一种新类型A, 来区分其独特的数据访问和执行模式。

## 5.2 GIRs中的自动差分

在编译时，我们没有梯度的实际值，我们使用占位符来表示它，实际值要么由DL后端的自动区分系统提供，要么在运行时向后执行期间计算

我们从**整个以顶点为中心的计算的输出开始**。我们找到它的生产者，并生成**反向算子**，**利用输出及其梯度计算生产者输入的梯度**。如果一个输入已经具有梯度，则将生成一个附加的Add操作符来累积新计算的梯度。然后对每个输入进行递归处理

确保一个运算符的**所有下游运算符在其自身之前被区分**，以避免通过**跟踪其下游依赖项来反向传播部分聚合的梯度**。另外，在实现算子的倒向逻辑时，需要注意算子的图类型，遵循图类型推理规则，才能得到正确的梯度

例如，E-type操作符，我们需要摄取沿边聚合算子来计算S-type或D-type的梯度，我们在图6中描述了GAT的后向GIR。我们在图6中描述了GAT的后向GIR

## 5.3 代码生成和执行

**生成执行单元**。对于GIR，我们运行各种图形级优化来生成一个优化的计算图(§6)。其中，**Seastar算子融合是将计算图的多个算子合并为一个算子**。

然后，我们将计算图划分为融合和非融合执行单元，其中融合/非融合单元由一组融合/非融合算子组成。对于未融合的执行单元，我们直接在DL后端发出操作符。对于**每个融合单元，我们根据海星融合模板生成一个核，并进行编译**。我们根据执行单元中操作符之间的数据依赖性来确定执行单元之间的依赖性。在运行时，我们遵循依赖关系逐个执行单元。

**运行时执行。**在运行时，我们**将每个已编译的执行单元封装成一个用户定义函数**，以便插入DL后端(例如，通过继承PyTorch中的autograd函数)。当用户调用编译的以顶点为中心的6函数时。Seastar根据执行单元之间的依赖关系调度执行单元。**对于未编译的内核，Seastar只调用DL后端实现**。如果后续程序需要输出，则将其记录在张量映射中。**对于已编译的内核，我们在张量映射中查找它们所需的输入，调用已编译的内核并记录它们的输出**。在向后传播期间，DL后端调用执行单元的向后逻辑，并提供梯度作为输入。然后，Seastar获得控制权，并调用相应的向后执行单元。一旦不依赖于状态映射中的张量，我们就立即清除它们，以避免内存泄漏。

# 6 seastar优化

**从示踪和自分化引擎获得的计算图通常包含冗余或无用的计算**，最重要的是，**现有的GNN框架未能利用大量算子融合的机会**(§2.3)。为了解决这些问题，我们提出了海星算子融合及其核级设计。我们还使用常见子表达式消除、常量折叠和数学简化等经典优化来删除冗余/无用的计算

## 6.1 图和数据表示

首先介绍海星的图形和数据表示。GNN训练中使用的数据集通常由两部分组成:**稀疏有向图以及顶点和边的特征向量**。顶点/边特征表示为张量，其第一个维度由顶点/边id索引(从0开始)。对于图的表示，我们采用了**广泛使用的压缩稀疏行(CSR)格式**，因为它对稀疏图的低内存消耗和高访问效率。我们还为**向后传递存储了一个反向CSR格式**。图7显示了示例图的数据布局。

![image-20220526112232009](https://alittleasewolf.github.io/imgs/image-20220526112232009.png)

为了生成(相反的)CSR格式，我们按照**降序(出)排序顶点**。在§6.3中，**排序对于内核级优化是必要的**。

我们为DGL[59]后面的边id创建了一个单独的数组，它的长度与用于访问E-type类型tensor的顶点id数组长度相同。直接**使用顶点id数组的索引作为边id**来节省内存消耗可能很诱人，但是索引和边id之间的映射在反向CSR格式中是无效的。我们将在§6.3.4中更详细地讨论这个问题。

在GNN程序开始时，**根据输入数据集构造图形张量和特征张量**，然后将其与待训练GNN模型中的参数张量一起移入GPU内存。

## 6.2 seastar操作算子融合

**生产者-消费者关系中的两个经营者可以融合为一个经营者，仅且仅当不需要将中间结果写入GPU的全局内存，生产者的结果可以直接流水线传输给消费者**

TVM[17]将可熔合算子分为三类:单射injective、还原reduction和复数complex。然后它利用以下融合机会:注射-注射 injective-injective、注射-复位injective-reduction和复注射complex-injective。但是图卷积不属于任何一类，因为它的执行依赖于图。

**seastar模式**：为了为GNN工作负载设计一个通用的算子融合方案，我们描述了GNN模型，并观察到尽管GNN模型多种多样，但它们共享一个相似的执行模式，我们将其命名为SourceEdge-Aggregation star **(seastar)模式**

当计算以顶点为中心，且中心顶点周围有边时，图案为星形。我们用§6.1中给出的图表和数据布局来解释海星模式，并显示它与方程1的关系。gnn通常从处理类型S的顶点特征开始, 操作符根据vertex的id来访问s-type的输入，然后产生相应的输出$h_v^l or h_u^l$。然后输出由E-type的操作符产生，我们根据CSR索引中检索到的目的地、源和边id，在每条边上进行计算。具体来说，目标顶点的id被指定为它在顶点偏移数组中的位置。第k层顶点的source和edge ids可以被访问，存储在k和k+1层的offset数组中。然后我们使用这些id来访问存储在张量对应行的嵌入，然后对行使用运用e-type操作符，最后，根据目标顶点的id对沿边计算产生的值进行聚合，写到输出张量对应的行，结果可以使用Dropout层或激活层(如LeakyRelu)进一步处理。

**seastar的核融合机会。**三种模式，S-E，E-E，E-A。

- S-E，融合将Source阶段的生产者和Edge阶段的消费者合并为一个经营者。在边缘阶段，我们可以访问边缘id以及源顶点和目标顶点id。因此，我们可以推迟源阶段的计算到edge阶段。

  具体来说，在边缘阶段，我们根据顶点的id查询张量对应的行，并使用检索到的矢量执行源阶段的计算。现在每条边都有源阶段的矢量结果。然后，我们可以将Source级的输出直接提供给边缘级的下游操作员。

- E-E融合。因为它们需要相同的索引集来访问嵌入。在GAT的前向传递中的LeakyRelu和Exp操作可以被融合在下面E-E中。

- E-A融合，首先在边缘阶段进行计算，然后使用目标顶点的id进行聚合(例如，GAT前向传递中的Exp和AggSum运算符可以被融合)。

**鉴别海星模式**

为了自动识别计算图中的海星模式，我们将海星的有效融合总结为有限状态机，如图8所示。

![image-20220526114236581](https://alittleasewolf.github.io/imgs/image-20220526114236581.png)

我们使用图6中GAT的前向传递来说明融合算法。最初，Add操作符的状态为0，类型为type E，因此它在转换后移动到状态1，child operation leakyrelu也是type E，所有state1到他自身有一个有效的融合。因此，LeakyRelu保持在状态1，它可以与其父添加操作符融合。类似地，Exp和AggSum分别转换为状态1和状态2。它们可以与前面所有的算子融合。对于Div操作符，在拓扑排序中最近的父操作符是AggSum，它处于状态2。然而，DIV的类型是E，仅仅只在状态2且有效的转变为D时才实现。因此，Div没有与它的父节点融合，FSM被重新启动。

## 6.3 kernel级别优化

为了支持融合核的高效执行，我们需要仔细地将GPU的大规模并行性映射到计算和数据上。我们发现了三种类型的并行执行机会:**按特征并行(FP)、按顶点并行(VP)和按边并行(EP)**。FP是gnn与PageRank等传统图形处理工作负载最显著的区别，GPU图形处理系统不会利用FP。在GNN训练中，每个顶点都与一个包含成百上千个元素的嵌入向量(或张量)相关联。相反，在PageRank中只有一个PageRank值。因此，利用FP可以获得巨大的性能增益。

DGL将执行委托给mingun, mingun是一个图形操作的GPU执行引擎。其执行策略深受GPU图形处理系统的影响。**mingun利用EP和FP，首先将线程块分配给边缘，然后将块中的线程映射到嵌入维度**，一个关键步骤是确定一条边的目标顶点的id。分配给一条边的线程在顶点偏移数组上进行二值搜索，以查找该边所属的偏移范围。例如，对于图7左侧所示的样例图，边缘id为4的线程将发现范围[3,5)包含边缘，因此返回B作为目标顶点。

通过利用EP, DGL平衡线程和块之间的负载，而不考虑顶点倾角。然而，这种方法有两个关键的限制。首先，二进制搜索需要O(log(N))搜索结构，N是图的顶点数量。**顶点数量很大时，它会导致大量的开销**。其次，**数据局部性差。关联到同一顶点的边可以分配到不同的块**。因此，**原子指令是必要的**，以避免在写入同一目标顶点时的数据竞争，例如，在聚合的情况下。此外，**同一目标顶点的嵌入可能由不同的线程块加载**，这进一步增加了加载指令的数量。

Seastar提出**feature-adaptive group**(§6.3.1)来增加gpu的占用，并利用FP。海星利用VP和以地区为中心的执行策略(§6.3.2)，并利用**动态负载平衡(§6.3.3)**来解决真实图中的**顶点度偏度**。当**特征大小设置为16**时，我们在图9中概述了Seastars的设计。

### 6.3.1 特征自适应组

顶点的原始特征可能有几千个维度，但随着GNN层的叠加，隐藏特征缩小为更紧凑的表示，例如，一个或几个标量。当特征维度很大时，我们可以直接**分配一个或多个线程块来处理这些特征**(表示其维度为D). 每个块包含$2^k$次方个线程，k是$2^k <= D$的最大整数。块中的线程关注于特征向量中连续范围的值的计算，从而导致**合并内存访问和SIMT执行**(除了特征大小不能被线程数除时的少量剩余部分)。

但是，如果D比较小，每个块将由非常少的线程组成，这导致了严重的低GPU占用，因为硬件限制:一个流媒体多处理器可以**并发运行的块的数量**。例如，在1080Ti GPU上，每个块16个线程将使理论上的上界占用减少到25%。这促使我们仅根据特征维度减少分配给特征的线程数，同时保持块的大小足够大。提出了**特征自适应线程组**的抽象方法——一个FATgroup包含$2^k$以最好地适应特征维数的变化。**块大小固定为一个常数**(例如，通常采用**256块大小**)。在物理上，一个FAT组可以与其他组共享一个块，或者映射到一个或多个块。对于每个线程，**其组id可以通过将全局线程id除以块大小来计算**，其在FAT组中的线程id为余数.

### 6.3.2 Locality-Centric执行

海星利用VP技术，强调**数据局部性**。Seastar将**每个顶点分配到一个FAT组**，对于每个顶点，FAT组**逐边进行计算**——我们称之为**顶点-平行边-顺序**。当我们忽略沿边的并行性时，这看起来是不可取的，但由于以下原因，它实际上是可取的。首先，**并行沿边计算会导致用于聚合的层次或原子操作，从而显式或隐式地在线程之间引入频繁的同步**。相反，当线程顺序访问边时，它们可以在寄存器中**积累部分结果**，而不需要任何形式的同步。其次，**目标顶点的特征只能加载一次并存储在寄存器中**，从而产生了良好的数据局部性。这种方法基本上**减少了目标顶点的加载指令的数量，从边的数量减少到顶点的数量**，因为前者通常更大。

### 6.3.3 动态负载均衡

不同的FAT组在Seastar以位置为中心的方案中可能会出现倾斜加载，因为现实世界的图通常遵循幂律度分布。我们提出动态负载均衡来解决负载倾斜的问题。

- 动态调度，负载窃取是一种常用的负载均衡策略[7,10]。一个简单而直接的设计是使用“**持久线程**”的技巧，其中FAT组中的线程在一个无限循环中运行，并在当前执行完成后自动增加一个**全局顶点计数器**，直到没有顶点剩余。然而，**GPU内存上的原子操作会带来开销，当顶点数量很大时，开销可能会很大**。为了解决这个问题，我们利用GPU硬件上的**块调度器来**帮助我们进行调度，因为块调度器硬件的开销可以忽略不计。具体来说，我们启动尽可能多的块，因为它需要覆盖所有的顶点，并依赖GPU的块调度程序动态启动和退出块(因此称为动态调度)。在动态调度中，对高度高的顶点的耗时计算可以与较短的顶点重叠，从而减少了倾斜工作负载的影响。

- **度排序**。为了进一步减少不平衡负载带来的开销，我们建议根据顶点的度数对它们进行排序。等级排序的好处有两方面。首先，虽然高次顶点仍然存在，但我们知道它们是排序在顶点数组的前面的，我们可以更早地处理它们，以便它们的计算可以**更好地与大量低次顶点的执行重叠**。第二，排序顶点偏移数组中连续的顶点范围具有相似(如果不相同)度，当特征维度D比较小时，**分配给连续顶点的FAT组将来自同一个线程块**。在这个小范围内使用统一的工作负载有助于消除块内的负载不平衡。虽然排序会带来开销，但我们的关键观点是，对于gnn，图结构要么是**固定的**(用于完整的图训练)，要么**可以准备好独立于当前的训练迭代**(例如，在后台采样小批)。我们可以在**不减慢当前训练的情况下对顶点进行排序**。

- **块排序。**考虑到高度值的顶点聚集在数组的前面，Seastar使用**块调度的顺序**作为处理顶点的顺序:**较早调度的块在前面处理顶点**。然而，每个内核中可用的内置块id并不能直接提供保证，也就是说，id较小的块不一定会提前调度。幸运的是，根据其他人[35,40]和我们自己的基准测试得到的结果，在**块id和它被调度到1-D网格的时间之间确实存在很强的相关性**，这意味着我们可以**简单地使用块id并避免原子指令**。注意，违反观察到的相关性会轻微影响处理顺序，但不会影响处理的正确性。

综上所述，我们从**特征维**开始，分配一个FAT组对特征进行数据并行操作。然后，块中剩余的并行度(如果有的话)分配给顶点进行沿边计算和聚合。顶点是根据它们的**度数**排序的，我们可以使用原子的blockId或内置的blockId来调度顶点的执行。例如，如果feature的维度D是16，我们固定groupsize为16，假设我们将块大小设置为128(可调参数B)，该块将被分配8个相似度的顶点来处理。有效地，每**个32线程的经纱将被划分为工作在2个顶点**。我们启动的**块数量等于顶点数除以8**，以便在前一个块退出时，后续块可以动态地填充SM。这种策略实现了**块和warp内的负载平衡**、**特性上的合并内存访问以及使用寄存器的快速聚合**。我们设计了一个访问邻居特征向量的微基准来验证该设计的有效性，并与DGL的负载均衡方法进行了比较。Seastar的表现明显优于DGL，最大的reddit数据集的速度高达946倍。个别设计贡献的详细分解见§7。

算法1给出了海星的CUDA模板。对于每个线程，第1-3行计算其组id和组中的线程id。第8-14行执行边缘顺序执行。以Emit开头的行是指令占位符，将在代码生成过程中被操作符的特定实现替换。为了简化操作符的实现，我们抽象了操作符的数据访问步骤，这样开发人员只需要为**标量输入编写顺序代码**。

![image-20220526152827839](https://alittleasewolf.github.io/imgs/image-20220526152827839.png)

### 6.3.4 海星的反向传播

我们讨论了如何对海星进行逆向执行。**向前和向后的训练呈现出完美的对称:**如果向前的计算遵循**海星模式**，向后的也会通过。这可以通过使用数据流进行推理。在正向训练中，**每个中心顶点接收来自其邻接顶点的数据**。在反向传递中，**中心顶点的梯度需要按照边缘的反向方向发送给它的所有邻居**。但从它的邻居的观点，我们发现，在向后通过，它聚集了所有出去的边。也很容易验证向前传递的源(聚合)阶段中的源(目的)操作现在是向后传递的聚合(源)阶段。这意味着我们可以重用所有的设计，除了我们应该**翻转边线到边线和重新排序顶点**(这可以在预处理阶段完成)。但是我们需要小心处理**边缘id**。当我们翻转**CSR表示的方向后**，相同的边索引不再对应于原来的边。因此，我们需要记住前向传递中的边id，并将它们与顶点索引数组一起排序/翻转。

### 6.3.5 异构的海星

异构图带来了一种新的并行性:**边型并行**。边之间的聚合操作可能与边类型上的聚合不同。现有系统要么**逐个执行每种边缘类型**，要么使用batched_matrix_乘法(BMM)等**批处理操作**。后者回到**数据流编程**。

泛型层次聚合模式首先在**同一类型的边上进行聚合并存储结果**，然后**在边类型上进行聚合**。由于顺序执行沿边计算的原因是相同的，因此可能更倾向于**顺序地进行聚合**。我们在这里的主要观察是，常见的**层次聚合操作(如max、sum和min)允许使用顺序算法**。例如，边和边类型的聚合可以通过两个For循环进行:**外部循环使用max并迭代边类型，而内部循环迭代边**。这有效地将异质训练问题转化为同质训练问题。

为了实现这个设计，我们需要引入一个次要的**排序键——边类型**，来对每个顶点的边以及顶点的度数进行排序。然后我们可以展开外环，检测边缘类型的变化，并将其作为信号进行原始的外环积累。边类型与边id一起存储，可以使用边id进行索引。我们还考虑了一种更压缩的方式来存储边缘类型，使用类似CSR的方案。基本上，我们通过引入**类型偏移数组，在顶点偏移数组和顶点id数组之间添加了一个间接层**。直观地说，该方案通过在**同一类型的边之间共享边类型值来节省内存**。我们不选择这个方案有两个原因:**向前和向后传递都需要存储类型偏移**，而存储边类型和边id可以在两次传递中共享。

事实上，只有当数据集满足条件$N_e/N_t > 2$时，压缩格式才有用. $N_e$是图的边数，$N_t$是所有顶点的唯一类型的和。对于我们实验中使用的4个常用数据集，最高和最低的比值分别为1.923和1.385，表明使用边缘型数组是更好的选择。

# 7 实验评估

我们使用PyTorch 1.6.0作为DL后端运行sestar，并与DGL-0.4[59]和PyG-1.6.0[22]进行比较。DGL和PyG也使用PyTorch作为后端。我们使用DGL和PyG作为基线，因为它们是最流行和最先进的GNN培训框架。

我们使用了三种硬件平台。第一款配备Intel(R) Xeon(R) E5-2660 v4 CPU(56个逻辑CPU, 2.00GHz)， 256gb内存和NVIDIA GTX 1080Ti gpu，设备内存为11gb。第二款配备了NVIDIA GTX 2080Ti，设备内存为11gb。第三款配备了NVIDIA Tesla V100，设备内存为16gb。我们使用最高优化级别的CUDA 10.1编译生成的内核。注意，Seastar只使用一个GPU。

由于GAT[58]、GCN[37]、APPNP[38]和R-GCN[53]模型在学术界和产业界被广泛采用，我们对这些系统进行了评价。对于所有模型，我们在DGL实现中使用它们的默认配置。我们为每个模型创建了单元测试，以确保生成的内核生成的结果与DGL相同，从而验证内核的正确性。对于所有模型，我们对它们进行了200个epoch的训练，并报告每个epoch的平均训练时间。我们放弃了前三个时代GPU热身的时间。我们还报告了系统的内存使用峰值。

<br>

实验后续再补充

<br>